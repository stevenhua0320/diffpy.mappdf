{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map PDF protocol demo\n",
    "\n",
    "This demo is intended to show the capability and general workflow of the mapPDF protocol as described in [insert link to paper]. It is intended as a quick start solution for a novel python user who doesn't want to create a similar solution from scratch.\n",
    "\n",
    "## Introduction:\n",
    "\n",
    "The mapPDF protocol was developed as a responce to increasing data quantity during modern total scattering experiments. The main idea is to address the problem of disjointed analysis workflows where different aspects of data reduction and modelling end up in various locations and formats. For questions and help with mapPDF post questions at https://groups.google.com/forum/#!forum/diffpy-users\n",
    "\n",
    "This protocol attempts to simplify the process of handling large sets of experimental files and create one \"collection\" of information from which further analysis would be possible. The collection contains all of the essential metadata about the experiment, the data and the analysis results.\n",
    "\n",
    "The demo provides an example of a typical workflow, shows how to create and provides a few suggestions on how to use the \"collection\". It should be run from the demo directory in the mapPDF download folder and uses data in the example_data subdirectory.   This demo script may be adapted for your data.  To do this, copy and paste this demo ipynb and the mappdf_utils.py file to the directory containing the data, and make appropriate edits so the scripts can find your data.\n",
    "\n",
    "The current implementation is aimed at users of the XPD beamline at NSLSII but can be readily extended to other sources by the user through it's transparent structure and use of easily available python packages.\n",
    "\n",
    "### Overall workflow:\n",
    "\n",
    "1. Set up the paths to metadata and images / data files.\n",
    "2. Load the data into a \"collection\" dataframe including that information.\n",
    "3. Transform data to PDF\n",
    "4. Use pearsonr to screen the dataset.\n",
    "5. Set up a structure refinement and apply it to the dataset\n",
    "6. Visualize refined parameters.\n",
    "7. Adjust/improve model\n",
    "\n",
    "It was designed to work with output files from the XPD beamline at NSLSII.\n",
    "Output from other instruments can be used given the general metadata format is the same, or if the user is willing to adjust the import script. Once set up, the protocol is intended to work with minimal adjustments between the experiments.\n",
    "\n",
    "### Requirements:\n",
    "\n",
    "For this to run you will need:\n",
    "\n",
    "Python3 (e.g., Anaconda Python) with following packages: \n",
    "numpy, pandas, matplotlib, scipy\n",
    "\n",
    "You would also need the pdfGetX3 package installed (http://www.diffpy.org/products/pdfgetx3.html).\n",
    "\n",
    "\n",
    "### Installation:\n",
    "\n",
    "Make sure you have all the required packages installed into your python environment and have downloaded the demo folder containing 'mappdf_utils.py' together with the data files required for the demo located in 'example_data' folder. See above for details.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continue with the demo:\n",
    "\n",
    "### Main imports for mapPDF\n",
    "The demo utilizes a number of standard python packages as well as custom functions from 'mappdf_utils.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from diffpy.pdfgetx import PDFGetter, PDFConfig\n",
    "\n",
    "# utilities for mapPDF\n",
    "from mappdf_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define filepath to informations\n",
    "The following cells sets the paths to the text document with the metadata and points towards the location of the I(q) files. \n",
    "\n",
    "#### This is the only parameter that needs to change between datasets apart from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defined example files\n",
    "meta_data_file = 'example_data/metadata_output.txt'\n",
    "chi_files_dir = 'example_data/chi/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the parameters for PDF transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_params = dict(qmaxinst=22, qmin=0.8, qmax=17, dataformat='Qnm',\n",
    "                   rmin=0.0, rmax=100.0, rstep=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load raw I(Q) data\n",
    "Then we define which columns from the metadata file we are interested in by defining a qoi_columns list. A single function then reads in the desired information together with the corresponding '.chi' files. We then store everything in our collection dataframe 'df'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qoi_colmns  = ['diff_x', 'diff_y', 'filename', 'composition']\n",
    "\n",
    "df = mappdf_load_chi(meta_data_file, qoi_columns=qoi_colmns, root=chi_files_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform data to PDF\n",
    "We create G(r) from each entry in the collection 'df' using the parameters defined above. We designate file nr. 15 as our background as well as point for comparison for raw I(q) and G(r) using Pearsonr test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the data\n",
    "bgr = 15\n",
    "process_chi_df(df, pdf_params, \n",
    "               background=bgr, iq_pearson_data=bgr, gr_pearson_data=bgr);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the contents of the collection at any point using the standard function in the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the first 5 rows for example\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collection screening using Pearson maps for I(Q)\n",
    "We can check for similarity to our background file using standar matplotlib functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new figure:\n",
    "fig, ax = plt.subplots(1,1, figsize=(12, 5), sharey=True)\n",
    "\n",
    "\n",
    "# Plot the data :\n",
    "im = ax.scatter(df['diff_x'], df['diff_y'], c=df['iq_pearson'], vmin=.8, marker='s', s=1000)\n",
    "\n",
    "\n",
    "## Set axis labels and style\n",
    "fig.suptitle('Pearson Map on Iq', fontsize=20)\n",
    "cax = fig.add_axes((0.93,0.1,0.03,0.8))\n",
    "ax.set_ylabel('Motor y')\n",
    "ax.set_xlabel('Motor x')\n",
    "conf_label_size(ax, 18)\n",
    "conf_tick_size(ax, 16)\n",
    "fig.colorbar(im, cax=cax)\n",
    "plt.set_cmap('inferno')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, all datasets are very similar, since the amount of material is very low and the background signal dominates the measurement.\n",
    "\n",
    "### Pearson map of G(r)\n",
    "We can however examine the background subtracted and transformed to PDF datasets, using the same functions as before. Now there is clear contranst between signal from support material and the nanomaterial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create scatter plot with pearson map\n",
    "# Create new figure:\n",
    "fig, ax = plt.subplots(1,1, figsize=(12, 5), sharey=True)\n",
    "\n",
    "\n",
    "# Plot data:\n",
    "im = ax.scatter(df['diff_x'], df['diff_y'], c=df['gr_pearson'], vmin=.8, marker='s', s=1000)\n",
    "\n",
    "\n",
    "## Set axis labels and figure style:\n",
    "fig.suptitle('Pearson Map on Gr', fontsize=20)\n",
    "cax = fig.add_axes((0.93,0.1,0.03,0.8))\n",
    "ax.set_ylabel('Motor y')\n",
    "ax.set_xlabel('Motor x')\n",
    "conf_label_size(ax, 18)\n",
    "conf_tick_size(ax, 16)\n",
    "plt.set_cmap('inferno')\n",
    "fig.colorbar(im, cax=cax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF refinements\n",
    "Now that we have everything in one collection dataframe, it is easy to apply a fit recipe to each entry and append the results. The exact fit recipe will vary from one experiment to the other. The general approach however is similar to the initial steps of building the dataframe.\n",
    "\n",
    "For questions and help with mapPDF post questions at https://groups.google.com/forum/#!forum/diffpy-users\n",
    "\n",
    "### Load refinements from disk\n",
    "In order to save time for the demonstration purposes, the refinements to an FCC model were performed and the collection was saved to a file. Here we load a file containing the results called 'refined_params.csv to a collection dataframe called 'rf' and continue the demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = pd.read_csv('example_data/refined_params.csv')\n",
    "print(list(rf.keys())) # print the column entries in the refined parameters file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recreate the total collection dataframe by joining the refinement results with the collection we created earlier. For ease of use we call it by a variable name 'd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.concat([df, rf], axis=1, join='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a heatmap of rw values using python and matplotlib functionality\n",
    "\n",
    "We can now visualize the refined parameters for the dataset.\n",
    "\n",
    "Here is an example of the heatmap plot using simple interation over the collection. \n",
    "\n",
    "###### We will use matplotlib package to generate plots, however any kind of visualization software can be used after the collection has been saved as a .csv\n",
    "The collection can be saved to a text file using d.to_csv() function. See pandas documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.head(1) # show the first entry in the collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate the plot of model agreement score Rw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define the size of the figure plot\n",
    "plt.figure(figsize=(14,5))\n",
    "\n",
    "\n",
    "## perform the iteration\n",
    "plt.scatter(d['diff_x'], d['diff_y'], c=d['rw'], cmap='RdBu_r', vmin=0, vmax=1, marker='s', s=1000)\n",
    "\n",
    "    \n",
    "# add the colorbar\n",
    "plot = plt.colorbar()\n",
    "\n",
    "# Set labels\n",
    "plt.ylabel('y motor [mm]')\n",
    "plt.xlabel('x motor [mm]')\n",
    "plot.set_label('Refinement score (rw)')\n",
    "    \n",
    "# remove the box around the graph\n",
    "plt.gca().invert_xaxis()\n",
    "for spine in plt.gca().spines.values():\n",
    "    spine.set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate the filtered plot of crystallite size based on a criteria for Rw < 0.3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define the size of the figure plot\n",
    "plt.figure(figsize=(14,5.5))\n",
    "\n",
    "## perform the iteration\n",
    "where = d['rw']<0.3 # define the filter criteria\n",
    "plt.scatter(d['diff_x'][where], d['diff_y'][where], c=d['psize'][where], cmap = 'RdBu_r', vmin = 40, vmax = 150, marker = 's',s=1000)\n",
    "\n",
    "    \n",
    "# add the colorbar\n",
    "plot = plt.colorbar()\n",
    "\n",
    "#keep the limits as before\n",
    "plt.xlim(-7.2,10.1)\n",
    "plt.ylim(17.43,26)\n",
    "\n",
    "# Set labels\n",
    "plt.ylabel('y motor [mm]')\n",
    "plt.xlabel('x motor [mm]')\n",
    "plot.set_label('Crystallite size ($\\AA$)')\n",
    "    \n",
    "# remove the box around the graph\n",
    "plt.gca().invert_xaxis()\n",
    "for spine in plt.gca().spines.values():\n",
    "    spine.set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of multiple filtering:\n",
    "\n",
    "It is easy to define multiple criteria for filtering as well as making multiple 1D slices of the collection. Here we visualize the diffraction patterns for two arbitrary sets of refinements that we define ourselves. First we filter out all the diffraction patterns with better structural refinement results, with rw < 60%. Then, we isolate the rest, i.e. 60% < rw < 100% \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a figure\n",
    "fig, ((ax1,ax2)) = plt.subplots(2, 1, figsize=[14,12])\n",
    "offset_plot1=0\n",
    "offset_plot2=0\n",
    "\n",
    "\n",
    "## perform iteration and filtering by a criteria\n",
    "x = d['q']\n",
    "y = d['iq']\n",
    "rw = d['rw']\n",
    "\n",
    "for i in range(len(d)):\n",
    "    if np.all(rw[i] < 0.6):\n",
    "        ax1.plot(x[i], y[i] + offset_plot1, lw=1,alpha=0.6)\n",
    "        offset_plot1=offset_plot1+3\n",
    "        \n",
    "    elif np.all(rw[i] < 1):\n",
    "        ax2.plot(x[i], y[i] + offset_plot2, lw=1,alpha=0.6,color='#bb271c')\n",
    "        offset_plot2=offset_plot2+3\n",
    "        \n",
    "# set limits to focus on the part of the diffration part\n",
    "ax1.set_xlim(15,74)\n",
    "ax2.set_xlim(15,74)\n",
    "# set labels for x and y axes\n",
    "ax1.set_ylabel('I($\\mathrm{a.u.}$)');\n",
    "ax1.set_xlabel('Q($\\mathrm{nm^{-1}}$)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.iloc[[0]] # locate a particular entry in the collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot ideas:\n",
    "\n",
    "You can add other datasets for comparison and add annotations to create more informative figures. Here we add a background file to compare with the filtered contents of the collection from the previous plot to see the main differences between datasets that include material and those that dont."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load a reference file for comparison\n",
    "q, Iq = np.genfromtxt('./example_data/ref_background.txt', skip_header=4).T        \n",
    "\n",
    "#define a figure\n",
    "fig, ((ax1)) = plt.subplots(1, 1, figsize=[14,12])\n",
    "offset_plot1=0\n",
    "offset_plot2=0\n",
    "\n",
    "## perform iteration and filtering by a criteria\n",
    "x = d['q']\n",
    "y = d['iq']\n",
    "rw = d['rw']\n",
    "\n",
    "for i in range(len(d)):\n",
    "    if np.all(rw[i] < 0.6):\n",
    "        ax1.plot(x[i], y[i] + 650 + offset_plot1, lw=1,alpha=0.6)\n",
    "        #\n",
    "        # add a difference curve\n",
    "        ax1.plot(x[i], y[i]-np.interp(x[i],q,Iq)+600, lw=1) \n",
    "        offset_plot1=offset_plot1+3\n",
    "        \n",
    "    elif np.all(rw[i] < 1):\n",
    "        ax1.plot(x[i], y[i] + offset_plot2, lw=1,alpha=0.6,color='#bb271c')\n",
    "        #\n",
    "        # add a difference curve\n",
    "        ax1.plot(x[i], y[i]-np.interp(x[i],q,Iq),lw=1)\n",
    "        offset_plot2=offset_plot2+3\n",
    "\n",
    "#plot the reference file and adjust it to align with the data\n",
    "ax1.plot(q,Iq,lw=3,ls='--',label = 'Background',color='black')\n",
    "ax1.plot(q,Iq+650,lw=3,ls='--',color='black')\n",
    "\n",
    "# set style and labels\n",
    "ax1.set_xlim(5,74)\n",
    "ax1.set_ylim(-50,1030)\n",
    "plt.legend()\n",
    "ax1.set_ylabel('I($\\mathrm{a.u.}$)') \n",
    "ax1.set_xlabel('Q($\\mathrm{nm^{-1}}$)')\n",
    "\n",
    "# add annotations\n",
    "ax1.text(7, 500, 'All refined 1D I(Q) patterns with $\\mathrm{rw>60\\%}$', fontsize=24);\n",
    "ax1.text(7, 960, 'All refined 1D I(Q) patterns with $\\mathrm{rw<60\\%}$', fontsize=24);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
